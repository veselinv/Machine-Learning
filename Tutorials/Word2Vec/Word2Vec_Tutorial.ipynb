{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word2Vec_Tutorial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNIj8fbAkXhpaJUUUbCSW0c"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"DKmWQ57_vAlJ","colab_type":"code","outputId":"afccf322-8642-42ac-c665-52bd0cc7df98","executionInfo":{"status":"ok","timestamp":1580740810471,"user_tz":-120,"elapsed":1014,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Download the text corpus\n","\n","import urllib.request\n","\n","url = 'http://www.gutenberg.org/cache/epub/14066/pg14066.txt'\n","response = urllib.request.urlopen(url)\n","text_data = response.read()\n","text_corpus = str(text_data, 'utf-8')\n","\n","# print the number of symbols in the corpus\n","print(len(text_corpus))\n","\n","# check the first 300 symbols\n","text_corpus[0:300]"],"execution_count":2,"outputs":[{"output_type":"stream","text":["152084\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\ufeffThe Project Gutenberg eBook, Everyday Foods in War Time, by Mary Swartz\\r\\nRose\\r\\n\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this e'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"Qd20-ibcsEKA","colab_type":"code","outputId":"ea160b5b-674c-4b99-c33d-7fa9c3d40c7c","executionInfo":{"status":"ok","timestamp":1580740858263,"user_tz":-120,"elapsed":2168,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from keras.preprocessing import text\n","\n","# split the corpus to words, and remove special characters\n","all_words = text.text_to_word_sequence(text_corpus, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’‘')\n","\n","# see the first 25 words, and the total number of words in the corpus\n","print(all_words[0:25])\n","len(all_words)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'everyday', 'foods', 'in', 'war', 'time', 'by', 'mary', 'swartz', 'rose', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["25911"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"2mitB1732Lpq","colab_type":"code","outputId":"99396d3e-283f-4448-b4d8-92a041f4594b","executionInfo":{"status":"ok","timestamp":1580741773646,"user_tz":-120,"elapsed":900,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# leave only words with independent semantic value; i.e., remove stopwords\n","stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\",\n","              \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n","              \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n","              \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\",\n","              \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\",\n","              \"just\", \"don\", \"should\", \"now\"]\n","\n","words = [word for word in all_words if word not in stop_words]\n","\n","# see the total word length of the corpus, ex stopwords\n","print(len(words))\n","# see first 25 non-stopword words\n","print(words[0:25])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["15365\n","['\\ufeffthe', 'project', 'gutenberg', 'ebook', 'everyday', 'foods', 'war', 'time', 'mary', 'swartz', 'rose', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 're', 'use']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KF3uTdQirZZO","colab_type":"code","outputId":"f5fb9d35-2ea9-42df-b170-8a1d460ffb32","executionInfo":{"status":"ok","timestamp":1580741775914,"user_tz":-120,"elapsed":542,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# create an index for the words, where each unique word has a unique ID number (index)\n","# tokens are word + id\n","tokenizer = text.Tokenizer()\n","tokenizer.fit_on_texts(words)\n","\n","# create the forward and inverse dictionaries\n","word2id = tokenizer.word_index\n","id2word = {v:k for k, v in word2id.items()}\n","\n","# add an end-of-sentence token\n","vocab_size = len(word2id) + 1\n","\n","# display a few examples from the tokenizer dictionary\n","\n","wids = [word2id[w] for w in words]\n","print('Word Count:', len(wids))\n","print('Vocabulary Size:', vocab_size)\n","print('Vocabulary Sample:', list(word2id.items())[:30])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Word Count: 15365\n","Vocabulary Size: 3216\n","Vocabulary Sample: [('1', 1), ('2', 2), ('milk', 3), ('one', 4), ('food', 5), ('cup', 6), ('water', 7), ('4', 8), ('fat', 9), ('meat', 10), ('add', 11), ('gutenberg', 12), ('salt', 13), ('teaspoon', 14), ('may', 15), ('bread', 16), ('project', 17), ('sugar', 18), ('diet', 19), ('3', 20), ('cups', 21), ('butter', 22), ('well', 23), ('vegetables', 24), ('work', 25), ('foods', 26), ('flour', 27), ('corn', 28), ('tm', 29), ('much', 30)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FEjHrQUEs6g8","colab_type":"code","outputId":"470c7df8-fd71-4e84-a23f-7ba1519ee7e2","executionInfo":{"status":"ok","timestamp":1580742034081,"user_tz":-120,"elapsed":1734,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["from keras.preprocessing.sequence import skipgrams\n","import numpy as np\n","\n","# Skipgram with Negative Sampling\n","# generate skip-grams with the Keras utility, with a window size of 5-10?\n","# skip grams are pairs of words, with an associated target of 1, if the pair are\n","# found in the window size of each other, and 0 if they are not, and so the \n","# whole language model problem becomes a self-supervised algorithm\n","\n","skip_grams = [skipgrams(wids, vocabulary_size=vocab_size, window_size=2)]\n","\n","# view sample skip-grams\n","pairs, labels = np.asarray(skip_grams[0][0]), np.asarray(skip_grams[0][1])\n","for i in range(10):\n","    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n","          id2word[pairs[i][0]], pairs[i][0], \n","          id2word[pairs[i][1]], pairs[i][1], \n","          labels[i]))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(orange (662), lemon (172)) -> 1\n","(ten (664), especially (269)) -> 1\n","(half (31), hours (170)) -> 1\n","(department (413), sauté (2801)) -> 0\n","(pieces (175), layers (1567)) -> 0\n","(lean (368), advised (2100)) -> 0\n","(substitutes (195), lard (272)) -> 1\n","(syrup (75), calamity (2052)) -> 0\n","(half (31), equal (533)) -> 0\n","(cranberry (2767), date (575)) -> 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EH_MyNXqt2JT","colab_type":"code","outputId":"3f131bff-64c5-453c-e7bd-e25da247bc2c","executionInfo":{"status":"ok","timestamp":1580742115832,"user_tz":-120,"elapsed":872,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# create the model architecture\n","\n","import keras\n","from keras.layers import Dot\n","from keras.layers.core import Dense, Reshape\n","from keras.layers.embeddings import Embedding\n","from keras import layers\n","\n","# set the latent dimension size - this latent vector will encode the semantics\n","# of each word\n","embed_size = 100\n","\n","word_inputs = layers.Input([1,], name='word_input')\n","\n","context_inputs = layers.Input([1,], name='context_input')\n","\n","# the neural network trains the embedding layer, which contains the latent\n","# representations of all words in the corpus, so is 3215 x 100 in size\n","embed = Embedding(vocab_size, embed_size,\n","                         embeddings_initializer=\"glorot_uniform\",\n","                         input_length=1)\n","\n","# use the single embed layer for both the word and context embeddings\n","\n","word_embed = embed(word_inputs)\n","\n","context_embed = embed(context_inputs)\n","\n","word_reshape = Reshape((embed_size,))(word_embed)\n","\n","context_reshape = Reshape((embed_size,))(context_embed)\n","\n","# the Dot layer is the similarity measure, followed by a sigmoid activation\n","# as displayed in the diagram below\n","\n","merge = Dot(axes = 1, normalize = True)([word_reshape, context_reshape])\n","\n","dense = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(merge)\n","\n","model = keras.Model(inputs = [word_inputs, context_inputs], outputs = dense)\n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n","\n","# view model summary\n","model.summary()\n","\n","# visualize model structure\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n","                 rankdir='TB').create(prog='dot', format='svg'))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","word_input (InputLayer)         (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","context_input (InputLayer)      (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 1, 100)       321600      word_input[0][0]                 \n","                                                                 context_input[0][0]              \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 100)          0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 100)          0           embedding_1[1][0]                \n","__________________________________________________________________________________________________\n","dot_1 (Dot)                     (None, 1)            0           reshape_1[0][0]                  \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n","==================================================================================================\n","Total params: 321,602\n","Trainable params: 321,602\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg height=\"516pt\" viewBox=\"0.00 0.00 476.00 387.00\" width=\"635pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 472,-383 472,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139931219001584 -->\n<g class=\"node\" id=\"node1\">\n<title>139931219001584</title>\n<polygon fill=\"none\" points=\"14.5,-332.5 14.5,-378.5 224.5,-378.5 224.5,-332.5 14.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54.5\" y=\"-351.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"94.5,-332.5 94.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"94.5,-355.5 152.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"152.5,-332.5 152.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-363.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"152.5,-355.5 224.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-340.3\">(None, 1)</text>\n</g>\n<!-- 139931199715368 -->\n<g class=\"node\" id=\"node3\">\n<title>139931199715368</title>\n<polygon fill=\"none\" points=\"111.5,-249.5 111.5,-295.5 355.5,-295.5 355.5,-249.5 111.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"153.5\" y=\"-268.8\">Embedding</text>\n<polyline fill=\"none\" points=\"195.5,-249.5 195.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"195.5,-272.5 253.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"253.5,-249.5 253.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304.5\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"253.5,-272.5 355.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304.5\" y=\"-257.3\">(None, 1, 100)</text>\n</g>\n<!-- 139931219001584&#45;&gt;139931199715368 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139931219001584-&gt;139931199715368</title>\n<path d=\"M151.2554,-332.3799C164.2403,-322.9259 179.4157,-311.8772 193.1708,-301.8625\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"195.4953,-304.4995 201.5196,-295.784 191.3752,-298.8405 195.4953,-304.4995\" stroke=\"#000000\"/>\n</g>\n<!-- 139931219003712 -->\n<g class=\"node\" id=\"node2\">\n<title>139931219003712</title>\n<polygon fill=\"none\" points=\"242.5,-332.5 242.5,-378.5 452.5,-378.5 452.5,-332.5 242.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-351.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"322.5,-332.5 322.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"322.5,-355.5 380.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"351.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"380.5,-332.5 380.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416.5\" y=\"-363.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"380.5,-355.5 452.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416.5\" y=\"-340.3\">(None, 1)</text>\n</g>\n<!-- 139931219003712&#45;&gt;139931199715368 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139931219003712-&gt;139931199715368</title>\n<path d=\"M315.7446,-332.3799C302.7597,-322.9259 287.5843,-311.8772 273.8292,-301.8625\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"275.6248,-298.8405 265.4804,-295.784 271.5047,-304.4995 275.6248,-298.8405\" stroke=\"#000000\"/>\n</g>\n<!-- 139931199715088 -->\n<g class=\"node\" id=\"node4\">\n<title>139931199715088</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 225,-212.5 225,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"32.5\" y=\"-185.8\">Reshape</text>\n<polyline fill=\"none\" points=\"65,-166.5 65,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"65,-189.5 123,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"123,-166.5 123,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174\" y=\"-197.3\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"123,-189.5 225,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139931199715368&#45;&gt;139931199715088 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139931199715368-&gt;139931199715088</title>\n<path d=\"M199.7948,-249.3799C185.8824,-239.8367 169.6012,-228.6686 154.8927,-218.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"156.6704,-215.5544 146.4442,-212.784 152.7107,-221.3269 156.6704,-215.5544\" stroke=\"#000000\"/>\n</g>\n<!-- 139931199715424 -->\n<g class=\"node\" id=\"node5\">\n<title>139931199715424</title>\n<polygon fill=\"none\" points=\"243,-166.5 243,-212.5 468,-212.5 468,-166.5 243,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-185.8\">Reshape</text>\n<polyline fill=\"none\" points=\"308,-166.5 308,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"308,-189.5 366,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"366,-166.5 366,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417\" y=\"-197.3\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"366,-189.5 468,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"417\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139931199715368&#45;&gt;139931199715424 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139931199715368-&gt;139931199715424</title>\n<path d=\"M267.4838,-249.3799C281.5111,-239.8367 297.9269,-228.6686 312.7569,-218.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"314.976,-221.3028 321.2753,-212.784 311.0385,-215.5152 314.976,-221.3028\" stroke=\"#000000\"/>\n</g>\n<!-- 139931199716992 -->\n<g class=\"node\" id=\"node6\">\n<title>139931199716992</title>\n<polygon fill=\"none\" points=\"98.5,-83.5 98.5,-129.5 368.5,-129.5 368.5,-83.5 98.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"117.5\" y=\"-102.8\">Dot</text>\n<polyline fill=\"none\" points=\"136.5,-83.5 136.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"136.5,-106.5 194.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"194.5,-83.5 194.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.5\" y=\"-114.3\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" points=\"194.5,-106.5 368.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"281.5\" y=\"-91.3\">(None, 1)</text>\n</g>\n<!-- 139931199715088&#45;&gt;139931199716992 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139931199715088-&gt;139931199716992</title>\n<path d=\"M146.2052,-166.3799C160.1176,-156.8367 176.3988,-145.6686 191.1073,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"193.2893,-138.3269 199.5558,-129.784 189.3296,-132.5544 193.2893,-138.3269\" stroke=\"#000000\"/>\n</g>\n<!-- 139931199715424&#45;&gt;139931199716992 -->\n<g class=\"edge\" id=\"edge6\">\n<title>139931199715424-&gt;139931199716992</title>\n<path d=\"M321.5162,-166.3799C307.4889,-156.8367 291.0731,-145.6686 276.2431,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"277.9615,-132.5152 267.7247,-129.784 274.024,-138.3028 277.9615,-132.5152\" stroke=\"#000000\"/>\n</g>\n<!-- 139931199715480 -->\n<g class=\"node\" id=\"node7\">\n<title>139931199715480</title>\n<polygon fill=\"none\" points=\"142.5,-.5 142.5,-46.5 324.5,-46.5 324.5,-.5 142.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"194.5,-.5 194.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"194.5,-23.5 252.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"223.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"252.5,-.5 252.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288.5\" y=\"-31.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"252.5,-23.5 324.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 139931199716992&#45;&gt;139931199715480 -->\n<g class=\"edge\" id=\"edge7\">\n<title>139931199716992-&gt;139931199715480</title>\n<path d=\"M233.5,-83.3799C233.5,-75.1745 233.5,-65.7679 233.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"237.0001,-56.784 233.5,-46.784 230.0001,-56.784 237.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"dt87kU9Qy6Sk","colab_type":"code","outputId":"3d00680e-6a28-4323-9782-295b6941de1f","executionInfo":{"status":"ok","timestamp":1580742547889,"user_tz":-120,"elapsed":115289,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":869}},"source":["# train the model for 15 epochs\n","\n","batch_size = 64\n","epochs = 15\n","\n","model.fit(\n","    x=[pairs[:,0],pairs[:,1]],\n","    y=labels,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    verbose=1,\n","    validation_split=0.2,\n","    shuffle=True\n",")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 98326 samples, validate on 24582 samples\n","Epoch 1/15\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","98326/98326 [==============================] - 8s 81us/step - loss: 0.6726 - val_loss: 0.6596\n","Epoch 2/15\n","98326/98326 [==============================] - 7s 73us/step - loss: 0.5651 - val_loss: 0.6457\n","Epoch 3/15\n","98326/98326 [==============================] - 7s 73us/step - loss: 0.4735 - val_loss: 0.6455\n","Epoch 4/15\n","98326/98326 [==============================] - 7s 75us/step - loss: 0.4038 - val_loss: 0.6529\n","Epoch 5/15\n","98326/98326 [==============================] - 8s 77us/step - loss: 0.3515 - val_loss: 0.6671\n","Epoch 6/15\n","98326/98326 [==============================] - 8s 77us/step - loss: 0.3119 - val_loss: 0.6843\n","Epoch 7/15\n","98326/98326 [==============================] - 8s 79us/step - loss: 0.2819 - val_loss: 0.7008\n","Epoch 8/15\n","98326/98326 [==============================] - 8s 78us/step - loss: 0.2591 - val_loss: 0.7177\n","Epoch 9/15\n","98326/98326 [==============================] - 8s 79us/step - loss: 0.2414 - val_loss: 0.7350\n","Epoch 10/15\n","98326/98326 [==============================] - 8s 76us/step - loss: 0.2274 - val_loss: 0.7494\n","Epoch 11/15\n","98326/98326 [==============================] - 8s 76us/step - loss: 0.2166 - val_loss: 0.7618\n","Epoch 12/15\n","98326/98326 [==============================] - 8s 80us/step - loss: 0.2079 - val_loss: 0.7745\n","Epoch 13/15\n","98326/98326 [==============================] - 8s 79us/step - loss: 0.2004 - val_loss: 0.7866\n","Epoch 14/15\n","98326/98326 [==============================] - 8s 80us/step - loss: 0.1947 - val_loss: 0.7960\n","Epoch 15/15\n","98326/98326 [==============================] - 8s 80us/step - loss: 0.1875 - val_loss: 0.8123\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f448ec07fd0>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"1oPe8lvNzJjc","colab_type":"code","outputId":"69a20e2d-89fe-4147-c019-887b7684aacb","executionInfo":{"status":"ok","timestamp":1580742617184,"user_tz":-120,"elapsed":1033,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# check the created embeddings as a dataframe\n","\n","import pandas as pd\n","\n","# get the weights by extracting the embedding layer from the trained model\n","weights = model.layers[-5].get_weights()[0][1:]\n","\n","print(weights.shape)\n","pd.DataFrame(weights, index=id2word.values()).head()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(3215, 100)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.072909</td>\n","      <td>0.019825</td>\n","      <td>-0.007141</td>\n","      <td>0.060088</td>\n","      <td>-0.051375</td>\n","      <td>0.190954</td>\n","      <td>0.125357</td>\n","      <td>0.149467</td>\n","      <td>0.175571</td>\n","      <td>-0.095964</td>\n","      <td>0.075636</td>\n","      <td>0.018884</td>\n","      <td>0.088015</td>\n","      <td>0.051566</td>\n","      <td>-0.111871</td>\n","      <td>-0.219616</td>\n","      <td>0.013046</td>\n","      <td>-0.119471</td>\n","      <td>0.129561</td>\n","      <td>0.195791</td>\n","      <td>-0.068884</td>\n","      <td>-0.037262</td>\n","      <td>0.113075</td>\n","      <td>-0.139861</td>\n","      <td>-0.008489</td>\n","      <td>-0.139104</td>\n","      <td>-0.018023</td>\n","      <td>-0.114215</td>\n","      <td>-0.097435</td>\n","      <td>-0.162742</td>\n","      <td>0.158162</td>\n","      <td>0.200547</td>\n","      <td>0.067446</td>\n","      <td>0.044481</td>\n","      <td>-0.160627</td>\n","      <td>-0.065632</td>\n","      <td>-0.145334</td>\n","      <td>-0.174004</td>\n","      <td>0.110052</td>\n","      <td>-0.020316</td>\n","      <td>...</td>\n","      <td>-0.023981</td>\n","      <td>-0.219380</td>\n","      <td>0.078180</td>\n","      <td>-0.217300</td>\n","      <td>0.085041</td>\n","      <td>-0.182830</td>\n","      <td>-0.075914</td>\n","      <td>-0.120019</td>\n","      <td>0.046566</td>\n","      <td>0.047690</td>\n","      <td>-0.156200</td>\n","      <td>-0.046808</td>\n","      <td>-0.221729</td>\n","      <td>0.035525</td>\n","      <td>0.033256</td>\n","      <td>-0.204091</td>\n","      <td>-0.074184</td>\n","      <td>0.086690</td>\n","      <td>0.112138</td>\n","      <td>0.131340</td>\n","      <td>-0.062747</td>\n","      <td>-0.166148</td>\n","      <td>0.117966</td>\n","      <td>-0.122126</td>\n","      <td>-0.045149</td>\n","      <td>-0.167504</td>\n","      <td>-0.071733</td>\n","      <td>-0.137078</td>\n","      <td>0.248715</td>\n","      <td>0.213625</td>\n","      <td>-0.046861</td>\n","      <td>-0.182441</td>\n","      <td>0.080535</td>\n","      <td>-0.076507</td>\n","      <td>0.105959</td>\n","      <td>0.155848</td>\n","      <td>-0.085949</td>\n","      <td>0.060687</td>\n","      <td>-0.094231</td>\n","      <td>-0.180648</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.074744</td>\n","      <td>0.143358</td>\n","      <td>-0.093729</td>\n","      <td>0.169246</td>\n","      <td>-0.196397</td>\n","      <td>-0.086590</td>\n","      <td>-0.036246</td>\n","      <td>0.106903</td>\n","      <td>-0.133195</td>\n","      <td>0.158323</td>\n","      <td>0.101614</td>\n","      <td>0.118634</td>\n","      <td>-0.089872</td>\n","      <td>-0.066172</td>\n","      <td>0.074193</td>\n","      <td>0.080635</td>\n","      <td>0.073234</td>\n","      <td>0.104610</td>\n","      <td>-0.106709</td>\n","      <td>-0.072534</td>\n","      <td>-0.147650</td>\n","      <td>0.104185</td>\n","      <td>0.057151</td>\n","      <td>-0.123703</td>\n","      <td>-0.175238</td>\n","      <td>0.028815</td>\n","      <td>0.025760</td>\n","      <td>-0.002354</td>\n","      <td>-0.024067</td>\n","      <td>0.137016</td>\n","      <td>-0.125869</td>\n","      <td>-0.149297</td>\n","      <td>0.083858</td>\n","      <td>0.088689</td>\n","      <td>-0.066314</td>\n","      <td>-0.087912</td>\n","      <td>-0.037630</td>\n","      <td>0.121791</td>\n","      <td>-0.050397</td>\n","      <td>0.055166</td>\n","      <td>...</td>\n","      <td>0.127436</td>\n","      <td>0.174135</td>\n","      <td>0.131380</td>\n","      <td>-0.027441</td>\n","      <td>-0.017621</td>\n","      <td>0.129132</td>\n","      <td>0.141263</td>\n","      <td>0.139219</td>\n","      <td>0.087059</td>\n","      <td>0.067882</td>\n","      <td>0.095847</td>\n","      <td>-0.131302</td>\n","      <td>-0.010714</td>\n","      <td>-0.046004</td>\n","      <td>0.117719</td>\n","      <td>-0.079638</td>\n","      <td>0.020650</td>\n","      <td>-0.074060</td>\n","      <td>0.073518</td>\n","      <td>0.084540</td>\n","      <td>0.151786</td>\n","      <td>0.102925</td>\n","      <td>0.038106</td>\n","      <td>0.019898</td>\n","      <td>0.071206</td>\n","      <td>0.006687</td>\n","      <td>0.073693</td>\n","      <td>-0.088099</td>\n","      <td>0.015728</td>\n","      <td>-0.149281</td>\n","      <td>0.108862</td>\n","      <td>0.015630</td>\n","      <td>0.130706</td>\n","      <td>0.054496</td>\n","      <td>-0.142024</td>\n","      <td>0.162879</td>\n","      <td>0.063987</td>\n","      <td>0.097060</td>\n","      <td>-0.151041</td>\n","      <td>-0.089756</td>\n","    </tr>\n","    <tr>\n","      <th>milk</th>\n","      <td>-0.106486</td>\n","      <td>0.085986</td>\n","      <td>0.057053</td>\n","      <td>0.127669</td>\n","      <td>0.108930</td>\n","      <td>0.030028</td>\n","      <td>-0.116592</td>\n","      <td>-0.142169</td>\n","      <td>0.131443</td>\n","      <td>0.174510</td>\n","      <td>-0.077262</td>\n","      <td>-0.120283</td>\n","      <td>-0.127833</td>\n","      <td>-0.142838</td>\n","      <td>0.136057</td>\n","      <td>-0.114417</td>\n","      <td>0.147386</td>\n","      <td>-0.122508</td>\n","      <td>0.055848</td>\n","      <td>0.136342</td>\n","      <td>0.013556</td>\n","      <td>-0.151769</td>\n","      <td>-0.099303</td>\n","      <td>0.070902</td>\n","      <td>0.070390</td>\n","      <td>-0.108092</td>\n","      <td>0.071684</td>\n","      <td>-0.130982</td>\n","      <td>-0.038978</td>\n","      <td>-0.130484</td>\n","      <td>-0.128876</td>\n","      <td>-0.179958</td>\n","      <td>-0.100698</td>\n","      <td>0.156909</td>\n","      <td>0.140357</td>\n","      <td>0.114915</td>\n","      <td>0.010765</td>\n","      <td>-0.050475</td>\n","      <td>0.082171</td>\n","      <td>0.171060</td>\n","      <td>...</td>\n","      <td>0.043872</td>\n","      <td>0.113975</td>\n","      <td>-0.056572</td>\n","      <td>-0.094534</td>\n","      <td>0.115408</td>\n","      <td>0.160483</td>\n","      <td>-0.115920</td>\n","      <td>0.116151</td>\n","      <td>-0.105829</td>\n","      <td>0.199499</td>\n","      <td>-0.090375</td>\n","      <td>0.125086</td>\n","      <td>-0.102607</td>\n","      <td>-0.165200</td>\n","      <td>-0.189712</td>\n","      <td>-0.043247</td>\n","      <td>-0.029866</td>\n","      <td>0.067722</td>\n","      <td>-0.056339</td>\n","      <td>0.002330</td>\n","      <td>-0.139084</td>\n","      <td>0.087619</td>\n","      <td>-0.175278</td>\n","      <td>-0.088049</td>\n","      <td>0.213401</td>\n","      <td>-0.153408</td>\n","      <td>0.088215</td>\n","      <td>0.100418</td>\n","      <td>-0.145435</td>\n","      <td>-0.162522</td>\n","      <td>-0.149494</td>\n","      <td>-0.029465</td>\n","      <td>-0.155037</td>\n","      <td>0.119490</td>\n","      <td>0.074175</td>\n","      <td>0.109793</td>\n","      <td>0.086686</td>\n","      <td>0.127584</td>\n","      <td>0.032139</td>\n","      <td>0.093683</td>\n","    </tr>\n","    <tr>\n","      <th>one</th>\n","      <td>0.158439</td>\n","      <td>-0.092404</td>\n","      <td>0.148978</td>\n","      <td>0.131154</td>\n","      <td>-0.123054</td>\n","      <td>-0.128632</td>\n","      <td>0.111148</td>\n","      <td>0.071838</td>\n","      <td>-0.103850</td>\n","      <td>0.136648</td>\n","      <td>-0.184108</td>\n","      <td>0.061584</td>\n","      <td>-0.003689</td>\n","      <td>0.053324</td>\n","      <td>-0.133177</td>\n","      <td>0.129539</td>\n","      <td>-0.111082</td>\n","      <td>0.081830</td>\n","      <td>0.116664</td>\n","      <td>-0.090770</td>\n","      <td>0.130520</td>\n","      <td>-0.160997</td>\n","      <td>0.112464</td>\n","      <td>-0.072035</td>\n","      <td>-0.058468</td>\n","      <td>0.078565</td>\n","      <td>0.167758</td>\n","      <td>-0.109367</td>\n","      <td>-0.071320</td>\n","      <td>0.126953</td>\n","      <td>-0.115932</td>\n","      <td>-0.076941</td>\n","      <td>0.130398</td>\n","      <td>0.057262</td>\n","      <td>-0.083738</td>\n","      <td>0.127319</td>\n","      <td>0.095782</td>\n","      <td>-0.149758</td>\n","      <td>0.079636</td>\n","      <td>-0.065653</td>\n","      <td>...</td>\n","      <td>0.085915</td>\n","      <td>0.055365</td>\n","      <td>0.018986</td>\n","      <td>0.038781</td>\n","      <td>0.144785</td>\n","      <td>-0.112182</td>\n","      <td>-0.162147</td>\n","      <td>0.095942</td>\n","      <td>-0.007149</td>\n","      <td>0.124770</td>\n","      <td>-0.206521</td>\n","      <td>-0.110223</td>\n","      <td>0.050844</td>\n","      <td>0.089666</td>\n","      <td>0.156417</td>\n","      <td>-0.052048</td>\n","      <td>0.124091</td>\n","      <td>0.158597</td>\n","      <td>-0.127662</td>\n","      <td>0.186651</td>\n","      <td>-0.081284</td>\n","      <td>-0.113675</td>\n","      <td>-0.061295</td>\n","      <td>0.108736</td>\n","      <td>-0.212406</td>\n","      <td>0.103784</td>\n","      <td>0.210104</td>\n","      <td>0.129839</td>\n","      <td>0.122294</td>\n","      <td>0.171327</td>\n","      <td>-0.086047</td>\n","      <td>0.050370</td>\n","      <td>-0.126694</td>\n","      <td>0.177628</td>\n","      <td>0.141252</td>\n","      <td>0.159731</td>\n","      <td>-0.063557</td>\n","      <td>0.147363</td>\n","      <td>-0.070246</td>\n","      <td>-0.137882</td>\n","    </tr>\n","    <tr>\n","      <th>food</th>\n","      <td>-0.108327</td>\n","      <td>-0.118577</td>\n","      <td>0.154358</td>\n","      <td>-0.071963</td>\n","      <td>0.028845</td>\n","      <td>0.126713</td>\n","      <td>0.153549</td>\n","      <td>-0.075503</td>\n","      <td>0.124387</td>\n","      <td>0.123777</td>\n","      <td>0.071890</td>\n","      <td>0.129450</td>\n","      <td>-0.131705</td>\n","      <td>0.064077</td>\n","      <td>0.062768</td>\n","      <td>0.125767</td>\n","      <td>-0.136856</td>\n","      <td>0.055710</td>\n","      <td>-0.130055</td>\n","      <td>-0.229728</td>\n","      <td>0.082524</td>\n","      <td>-0.065505</td>\n","      <td>-0.168403</td>\n","      <td>-0.078442</td>\n","      <td>-0.116890</td>\n","      <td>0.100552</td>\n","      <td>-0.095908</td>\n","      <td>-0.124989</td>\n","      <td>-0.097059</td>\n","      <td>-0.045402</td>\n","      <td>-0.154924</td>\n","      <td>0.084947</td>\n","      <td>-0.103485</td>\n","      <td>0.144466</td>\n","      <td>0.088254</td>\n","      <td>-0.093005</td>\n","      <td>-0.088046</td>\n","      <td>-0.126223</td>\n","      <td>0.102764</td>\n","      <td>-0.116977</td>\n","      <td>...</td>\n","      <td>-0.116706</td>\n","      <td>-0.152810</td>\n","      <td>-0.120789</td>\n","      <td>-0.076763</td>\n","      <td>-0.104078</td>\n","      <td>0.097883</td>\n","      <td>0.095252</td>\n","      <td>0.042234</td>\n","      <td>0.121632</td>\n","      <td>-0.113585</td>\n","      <td>-0.000391</td>\n","      <td>0.077066</td>\n","      <td>0.135586</td>\n","      <td>0.123473</td>\n","      <td>-0.062998</td>\n","      <td>-0.163973</td>\n","      <td>-0.169370</td>\n","      <td>-0.056973</td>\n","      <td>-0.059237</td>\n","      <td>-0.138699</td>\n","      <td>-0.147873</td>\n","      <td>-0.086515</td>\n","      <td>0.083842</td>\n","      <td>-0.158943</td>\n","      <td>0.153848</td>\n","      <td>-0.023379</td>\n","      <td>0.065416</td>\n","      <td>0.120349</td>\n","      <td>-0.024906</td>\n","      <td>-0.100247</td>\n","      <td>-0.164473</td>\n","      <td>-0.069726</td>\n","      <td>-0.091978</td>\n","      <td>0.005458</td>\n","      <td>-0.110539</td>\n","      <td>-0.190401</td>\n","      <td>0.063313</td>\n","      <td>-0.176020</td>\n","      <td>0.149787</td>\n","      <td>-0.146712</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 100 columns</p>\n","</div>"],"text/plain":["            0         1         2   ...        97        98        99\n","1    -0.072909  0.019825 -0.007141  ...  0.060687 -0.094231 -0.180648\n","2    -0.074744  0.143358 -0.093729  ...  0.097060 -0.151041 -0.089756\n","milk -0.106486  0.085986  0.057053  ...  0.127584  0.032139  0.093683\n","one   0.158439 -0.092404  0.148978  ...  0.147363 -0.070246 -0.137882\n","food -0.108327 -0.118577  0.154358  ... -0.176020  0.149787 -0.146712\n","\n","[5 rows x 100 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"85LWf-Jg5bb6","colab_type":"code","outputId":"502fab8b-5f5a-4700-e7c4-a6a115f2d575","executionInfo":{"status":"ok","timestamp":1580743049915,"user_tz":-120,"elapsed":911,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# The above is the quanitification of language semantics! (see presentation?)\n","# We can then do a few things, like check the top 5 closest words to a sample \n","\n","from sklearn.metrics.pairwise import euclidean_distances\n","\n","distance_matrix = euclidean_distances(weights)\n","print(distance_matrix.shape)\n","\n","similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n","                   for search_term in ['food', 'chef', 'war']}\n","\n","similar_words"],"execution_count":21,"outputs":[{"output_type":"stream","text":["(3215, 3215)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'chef': ['unsatisfactory', 'expend', 'ultimate', 'simon', '539'],\n"," 'food': ['facts', 'punitive', 'invariably', 'subscribe', 'observed'],\n"," 'war': ['allayed', 'depths', '12mo', 'pennsylvania', 'l']}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"8MEv_keYPzEj","colab_type":"code","outputId":"f6bddc43-6f95-4e72-ffea-26b50cbc8e0b","executionInfo":{"status":"ok","timestamp":1580743141995,"user_tz":-120,"elapsed":914,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["# do some bag of words math!\n","\n","def word_math(word1, word2, word3, num_words):\n","  new_word = weights[word2id[word1]] - weights[word2id[word2]] + weights[word2id[word3]]\n","  similarities = [np.dot(new_word, word_vec)/(np.linalg.norm(new_word)*np.linalg.norm(word_vec)) for word_vec in weights]\n","  closest = np.asarray(similarities).argsort()\n","  num_words = num_words\n","  closest_words = []\n","  for i in range(1, num_words + 1):\n","    close_word = id2word[closest[i - i*2]]\n","    closest_words.append(close_word)\n","  return closest_words\n","\n","word_math('soup','water','meat', 10)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['soup',\n"," 'country',\n"," 'accessible',\n"," 'alternative',\n"," 'brick',\n"," 'traditions',\n"," 'mistakes',\n"," 'eight',\n"," 'ascii',\n"," 'liability']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"NBHW4lh9dpDR","colab_type":"text"},"source":["#The pre-trained model"]},{"cell_type":"code","metadata":{"id":"zQVcbEpkc2Pq","colab_type":"code","outputId":"687e4734-03f7-4ae8-960a-70ce24d1f4db","executionInfo":{"status":"ok","timestamp":1580743206072,"user_tz":-120,"elapsed":46320,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# download a pre-trained model\n","\n","import spacy.cli\n","\n","#spacy.cli.download(\"en_core_web_lg\")\n","spacy.cli.download(\"en_core_web_md\")\n","nlp = spacy.load(\"en_core_web_md\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_LumLzQjanOE","colab_type":"code","outputId":"bc380bc3-1982-4b8d-dee6-d655dcfcc02a","executionInfo":{"status":"ok","timestamp":1580743228575,"user_tz":-120,"elapsed":2159,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# create the vector matrix; should contained ~700k words\n","\n","ids = [x for x in nlp.vocab.vectors.keys()]\n","vectors = [nlp.vocab.vectors[x] for x in ids]\n","vectors = np.array(vectors)\n","vectors.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(684830, 300)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"JnYoEWU4dtue","colab_type":"code","outputId":"b7912fb6-5a97-4a1d-ce8e-50179d3969a3","executionInfo":{"status":"ok","timestamp":1580743383818,"user_tz":-120,"elapsed":11319,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["# Do some more word math!\n","\n","def word_math(word1, word2, word3, num_words):\n","  new_word = nlp(word1).vector.astype('float64') - nlp(word2).vector.astype('float64') + nlp(word3).vector.astype('float64')\n","  similarities = [np.dot(new_word, word_vec)/(np.linalg.norm(new_word)*np.linalg.norm(word_vec)) for word_vec in vectors]\n","  closest = np.asarray(similarities).argsort()\n","  num_words = num_words\n","  closest_words = []\n","  for i in range(1, num_words + 1):\n","    close_word = ids[closest[i - i*2]]\n","    closest_words.append(nlp.vocab[close_word].text)\n","  return closest_words\n","\n","word_math('soup','water','meat', 20)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['MEAT',\n"," 'Meat',\n"," 'meat',\n"," 'chowder',\n"," 'minestrone',\n"," 'Soup',\n"," 'MINESTRONE',\n"," 'Chowder',\n"," 'vichyssoise',\n"," 'soup',\n"," 'VICHYSSOISE',\n"," 'CHOWDER',\n"," 'Minestrone',\n"," 'SOUP',\n"," 'Vichyssoise',\n"," 'Beef',\n"," 'extra-lean',\n"," 'BEEF',\n"," 'Extra-Lean',\n"," 'grassfed']"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"790mva7DkYDo","colab_type":"text"},"source":["# Text Classification\n","# ChatBots\n","# Sentiment Analysis\n","# Semantic Search\n","# Text Summary, Text Generation, Question Answering, etc."]},{"cell_type":"code","metadata":{"id":"RgpejU2nmIcU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}