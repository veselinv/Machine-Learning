{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble Pipeline.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"D-NmvtmR6Kts","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fX0Utm5S6IM8","colab_type":"code","colab":{}},"source":["#Import all the libraries\n","\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import xgboost as xgb\n","from scipy import stats\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.impute import MissingIndicator, SimpleImputer\n","from sklearn.metrics import r2_score, mean_squared_error\n","from sklearn.linear_model import Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.feature_selection import SelectKBest, f_regression\n","from sklearn.pipeline import Pipeline,FeatureUnion\n","from math import sqrt\n","import random\n","import time\n","import os\n","import shutil"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Js0AMowh6INB","colab_type":"code","colab":{}},"source":["#import the training file and split for train and test\n","\n","df = pd.read_excel('./drive/My Drive/Colab Notebooks/Regression/Train Data.xlsx', encoding='latin-1')\n","df_train, df_test = train_test_split(df, train_size = 0.8, test_size = 0.2, random_state = 7)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhilC5zA6INF","colab_type":"code","colab":{}},"source":["#create a class to replace the 'NA' with NaN\n","\n","class Replace_NAs(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        return self\n","    def transform(self, X, y=None):\n","        X.replace({'N/A': ''}, regex=True)\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtjAzZZO6INI","colab_type":"code","colab":{}},"source":["#create classes for selecting the various data types for preprocessing\n","\n","class Date_Select(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.feature_names = []\n","        return self\n","    def transform(self, X, y=None):\n","        X = X.select_dtypes(include=['datetime'])\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names\n","\n","class Cat_Select(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.feature_names = []\n","        return self\n","    def transform(self, X, y=None):\n","        X = X.select_dtypes(include=['object', 'bool'])\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names\n","    \n","\n","class Num_Select(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.feature_names = []\n","        return self\n","    def transform(self, X, y=None):\n","        X = X.select_dtypes(include=['int64','float64'])\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEgIJh5n6INL","colab_type":"code","colab":{}},"source":["# create a transformer to extract the month and year from the dates\n","\n","class Date_Extract(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.feature_names = []\n","        return self\n","    def transform(self, X, y=None):\n","        m, y, wd = 1, 1, 1\n","        for date in X.columns:\n","            X[date+'_m'], X[date+'_y'], X[date+'_wd'] = X[date].dt.month, X[date].dt.year, X[date].dt.weekday\n","            m = m + 1\n","            y = y + 1\n","            wd = wd + 1\n","            X = X.drop([date],axis=1)\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QXePh5J6INP","colab_type":"code","colab":{}},"source":["#create a class to fill the missing values in the categorical features (with the name of the feature and prefix '_other'\n","\n","class Fill_CatNaNs(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.feature_names = []\n","        return self\n","    def transform(self, X, y=None):\n","        X = X.fillna(value = pd.concat([pd.DataFrame(X.columns[X.isna().any()],columns = ['Dict']),\n","                'other_' + pd.DataFrame(X.columns[X.isna().any()]).astype(str)],\n","                axis=1).set_index('Dict').to_dict()[0])\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_aok_Gb6INT","colab_type":"code","colab":{}},"source":["#create a custom class for the transformer classes, to keep column names\n","\n","class Custom_Indicator(TransformerMixin,BaseEstimator):\n","    \n","    Indicator = MissingIndicator(missing_values=np.NaN, error_on_new = False)\n","    feature_names = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        self.feature_names = []\n","        self.Indicator.fit(X)\n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Indicator.transform(X), index = X.index).add_prefix('ind_')\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names\n","    \n","class Custom_Imputer(TransformerMixin,BaseEstimator):\n","    \n","    Imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n","    feature_names = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        self.feature_names = []\n","        self.Imputer.fit(X)\n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Imputer.transform(X), index = X.index, columns = X.columns)\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names\n","\n","class Custom_Scaler(TransformerMixin,BaseEstimator):\n","    \n","    Scaler = MinMaxScaler()\n","    feature_names = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        self.feature_names = []\n","        self.Scaler.fit(X)\n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Scaler.transform(X), index = X.index, columns = X.columns)\n","        self.feature_names = X.columns.tolist()\n","        return X\n","    def get_feature_names(self):\n","        return self.feature_names\n","    \n","class Custom_Encoder(TransformerMixin,BaseEstimator):\n","    \n","    Encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')\n","    ohe_features = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        self.ohe_features = []\n","        self.Encoder.fit(X)\n","        self.ohe_features = self.Encoder.get_feature_names()\n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Encoder.transform(X), index = X.index, columns = self.ohe_features)\n","        return X\n","    def get_feature_names(self):\n","        return self.ohe_features\n","    \n","class Date_Encoder(TransformerMixin,BaseEstimator):\n","    \n","    Encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')\n","    ohe_features = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        self.ohe_features = []\n","        self.Encoder.fit(X)\n","        self.ohe_features = self.Encoder.get_feature_names()\n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Encoder.transform(X), index = X.index, columns = self.ohe_features)\n","        return X\n","    def get_feature_names(self):\n","        return self.ohe_features\n","\n","class Custom_Selector(TransformerMixin,BaseEstimator):\n","    \n","    Selector = SelectKBest(f_regression, k=64)\n","    column_names = []\n","    \n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        self.column_names = []\n","        self.Selector.fit(X,y)\n","        mask = self.Selector.get_support()\n","        for bool, feature in zip(mask, list(X.columns.values)):\n","            if bool:\n","                self.column_names.append(feature)     \n","        return self\n","    def transform(self, X, y=None):\n","        X = pd.DataFrame(data = self.Selector.transform(X), index = X.index, columns = self.column_names)\n","        return X\n","    def get_feature_names(self):\n","        return self.column_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARf4iQQA6INX","colab_type":"code","colab":{}},"source":["#define the steps in the primary pipelines\n","\n","MissingInd_Pipe = Pipeline(steps = [('Num_Select', Num_Select()),\n","                                    ('Imp_Indicate', Custom_Indicator())])\n","\n","Numerical_Pipe = Pipeline(steps = [('Num_Select', Num_Select()),\n","                                   ('Imputer', Custom_Imputer()),\n","                                   ('Scaler', Custom_Scaler())])\n","\n","Date_Pipe = Pipeline(steps = [('Date_Select',Date_Select()),\n","                              ('Date_Extract',Date_Extract()),\n","                             ('Date_Encode',Date_Encoder())])\n","\n","Cat_Pipe = Pipeline(steps = [('Cat_Select',Cat_Select()),\n","                             ('Fill_CatNaNs',Fill_CatNaNs()),\n","                             ('Encoder', Custom_Encoder())])\n","\n","Union = FeatureUnion([('Missing_Pipe',MissingInd_Pipe),\n","                      ('Numerical_Pipe',Numerical_Pipe),\n","                      ('Date_Pipe', Date_Pipe),\n","                      ('Cat_Pipe',Cat_Pipe)])\n","\n","features_all = Pipeline([#('replace_nas', Replace_NAs()),\n","                         ('all_features',FeatureUnion([('Missing_Pipe',MissingInd_Pipe),\n","                                                ('Numerical_Pipe',Numerical_Pipe),\n","                                                ('Date_Pipe', Date_Pipe),\n","                                                ('Cat_Pipe',Cat_Pipe)]))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvzVmvrs6INb","colab_type":"code","colab":{}},"source":["class Feature_Names(TransformerMixin,BaseEstimator):\n","    feature_names = []\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y):\n","        return self\n","    def transform(self, X, y=None):\n","        X1 = pd.DataFrame(data = X, columns = np.concatenate((features_all.named_steps['all_features'].transformer_list[0][1].named_steps['Imp_Indicate'].get_feature_names(),\n","                                                             features_all.named_steps['all_features'].transformer_list[1][1].named_steps['Scaler'].get_feature_names(),\n","                                                             features_all.named_steps['all_features'].transformer_list[2][1].named_steps['Date_Encode'].get_feature_names(),\n","                                                             features_all.named_steps['all_features'].transformer_list[3][1].named_steps['Encoder'].get_feature_names()),\n","                                                            axis = None))\n","        return X1\n","    def get_feature_names(self):\n","        return self.feature_names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDMwDCwH6INf","colab_type":"code","colab":{}},"source":["named_features = Pipeline([('feats',Union),\n","                         ('feat_names', Feature_Names()),\n","                         ('Selector',Custom_Selector())])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-O5L_Tp46INk","colab_type":"code","colab":{}},"source":["#Create the data reader to read and batch the npz files\n","\n","class SP_Data_Reader():\n","    # Dataset is a mandatory arugment, while the batch_size is optional\n","    # If you don't input batch_size, it will automatically take the value: None\n","    def __init__(self, dataset, batch_size = None):\n","    \n","        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n","        # e.g. if I call this class with x('train',5), it will load 'SP_Data_train.npz' with a batch size of 5.\n","        npz = np.load('SP_data_{0}.npz'.format(dataset))\n","\n","        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.float)\n","\n","        if batch_size is None:\n","            self.batch_size = self.inputs.shape[0]\n","        else:\n","            self.batch_size = batch_size\n","        self.curr_batch = 0\n","        self.batch_count = self.inputs.shape[0] // self.batch_size\n","    \n","    def __next__(self):\n","        if self.curr_batch >= self.batch_count:\n","            self.curr_batch = 0\n","            raise StopIteration()\n","            \n","        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n","        inputs_batch = self.inputs[batch_slice]\n","        targets_batch = self.targets[batch_slice]\n","        self.curr_batch += 1\n","        \n","        return inputs_batch, targets_batch\n","    \n","    def __iter__(self):\n","        return self"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keIKLxk26INp","colab_type":"code","colab":{}},"source":["#create a class fit, test and predict with the primary DNN model\n","\n","class Primary_DNN(TransformerMixin,BaseEstimator):\n","    def __init__(self, name):\n","        self.name = name\n","        pass\n","    def fit(self, X, y):\n","        \n","        #split the primary data to train and validation, incl. startifying\n","        \n","        tf.reset_default_graph()\n","\n","        x_train, x_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 7)\n","        \n","        np.savez('SP_data_train',inputs = x_train,targets = y_train)\n","        np.savez('SP_data_validate',inputs = x_val,targets = y_val)\n","        \n","        # set the parameters for the primary DNN model\n","        \n","        primary_output_size = 1\n","        primary_input_size = x_train.shape[1]\n","        \n","        dropout_rate = random.choice([0.05, 0.1, 0.2])\n","        layers = random.randrange(3, 5)\n","        lr = random.choice([0.01, 0.001, 0.0001])\n","        funnel = random.choice([1, 2])\n","        shape = random.choice([256, 512, 1024, 2048])\n","        \n","        primary_inputs = tf.placeholder(tf.float32,[None, primary_input_size], name = 'primary_inputs')\n","        primary_targets = tf.placeholder(tf.float32,[None])\n","        \n","        primary_weights_1 = tf.get_variable(\"primary_weights_1\",[primary_input_size, shape])\n","        primary_biases_1 = tf.get_variable(\"primary_biases_1\",[shape])\n","        \n","        primary_drop_1 = tf.nn.dropout(primary_inputs, rate = dropout_rate)\n","        primary_outputs_1 = tf.nn.relu(tf.matmul(primary_drop_1,primary_weights_1)+primary_biases_1)\n","\n","        primary_weights_2 = tf.get_variable(\"primary_weights_2\",[shape, shape/funnel])\n","        primary_biases_2 = tf.get_variable(\"primary_biases_2\",[shape/funnel])\n","        \n","        primary_drop_2 = tf.nn.dropout(primary_outputs_1, rate = dropout_rate)\n","        primary_outputs_2 = tf.nn.relu(tf.matmul(primary_drop_2,primary_weights_2)+primary_biases_2)\n","\n","        primary_weights_3 = tf.get_variable(\"primary_weights_3\",[shape/funnel, shape/funnel**2])\n","        primary_biases_3 = tf.get_variable(\"primary_biases_3\",[shape/funnel**2])\n","        \n","        primary_drop_3 = tf.nn.dropout(primary_outputs_2, rate = dropout_rate)\n","        primary_outputs_3 = tf.nn.relu(tf.matmul(primary_drop_3,primary_weights_3)+primary_biases_3)\n","\n","        primary_weights_4 = tf.get_variable(\"primary_weights_4\",[shape/funnel**2, shape/funnel**3])\n","        primary_biases_4 = tf.get_variable(\"primary_biases_4\",[shape/funnel**3])\n","        \n","        primary_drop_4 = tf.nn.dropout(primary_outputs_3, rate = dropout_rate)\n","        primary_outputs_4 = tf.nn.relu(tf.matmul(primary_drop_4,primary_weights_4)+primary_biases_4)\n","\n","        primary_weights_5 = tf.get_variable(\"primary_weights_5\",[shape/funnel**3, primary_output_size])\n","        primary_biases_5 = tf.get_variable(\"primary_biases_5\",[primary_output_size])\n","        \n","        if layers == 3:\n","          primary_outputs = tf.matmul(primary_outputs_2,primary_weights_3)+primary_biases_3\n","        elif layers == 4:\n","          primary_outputs = tf.matmul(primary_outputs_3,primary_weights_4)+primary_biases_4\n","        elif layers == 5:\n","          primary_outputs = tf.matmul(primary_outputs_4,primary_weights_5)+primary_biases_5\n","      \n","        primary_predictions = tf.math.reduce_sum(primary_outputs, axis = 1, name = 'primary_predictions')\n","        \n","        #set the parameters for the primary DNN model\n","        #reduce_sum at final output to 'collapse' vector to single value\n","\n","        primary_loss = tf.losses.mean_squared_error(predictions = primary_predictions, labels = primary_targets)\n","        primary_mean_loss = tf.reduce_mean(primary_loss)\n","        primary_optimize = tf.train.AdamOptimizer(learning_rate=lr).minimize(primary_mean_loss)\n","        primary_accuracy = tf.reduce_mean(abs(primary_predictions - primary_targets), name = 'primary_accruacy')\n","        sess = tf.InteractiveSession()\n","        init_g = tf.global_variables_initializer()\n","        sess.run(init_g)\n","\n","        batch_size = 64\n","        max_epochs = 50\n","\n","        primary_train_data = SP_Data_Reader('train',batch_size)\n","        primary_validation_data = SP_Data_Reader('validate')\n","\n","        agg_validation_loss = [9999999.]\n","        primary_patience = -4\n","        \n","        #train the primary DNN model\n","        start_time = time.time()\n","        for epoch_counter in range(max_epochs):\n","            curr_epoch_loss = 0.\n","            for inputs_batch, targets_batch in primary_train_data:\n","                _, batch_loss = sess.run([primary_optimize, primary_mean_loss],feed_dict={primary_inputs: inputs_batch,\n","                                                                                          primary_targets: targets_batch})\n","                curr_epoch_loss = curr_epoch_loss + batch_loss\n","            curr_epoch_loss = curr_epoch_loss/primary_train_data.batch_count\n","\n","            primary_validation_loss = 0.\n","            primary_validation_accuracy = 0.\n","\n","            for inputs_batch, targets_batch in primary_validation_data:\n","                primary_validation_loss, primary_validation_accuracy = sess.run([primary_mean_loss,primary_accuracy],\n","                                                                                feed_dict={primary_inputs: inputs_batch,\n","                                                                                           primary_targets: targets_batch})\n","            \n","                print('Primary DNN'+str(self.name)+', Epoch: '+str(epoch_counter+1)+\n","                  '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n","                  '. Validation loss: '+'{0:.3f}'.format(primary_validation_loss)+\n","                  '. Validation accuracy abs: '+'{0:.2f}'.format(primary_validation_accuracy)+'m')\n","            \n","            #set the early stop mechanism and saving the best-performing model\n","            \n","            if primary_validation_loss < min(agg_validation_loss[:epoch_counter+1]):\n","                if os.path.isdir(r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary'+str(self.name)):\n","                    shutil.rmtree(r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary'+str(self.name))\n","\n","                primary_builder = tf.saved_model.builder.SavedModelBuilder \\\n","                (r'/content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary'+str(self.name))\n","                primary_builder.add_meta_graph_and_variables(sess,[\"tag\"],\n","                                                                 signature_def_map={\"model\":tf.saved_model.signature_def_utils.predict_signature_def(\n","                            inputs= {\"primary_inputs\": primary_inputs},\n","                            outputs= {\"primary_predictions\": primary_predictions})})\n","                primary_builder.save()\n","            \n","            agg_validation_loss.append(primary_validation_loss)\n","            \n","            if epoch_counter > abs(primary_patience):\n","                if min(agg_validation_loss[primary_patience:]) > min(agg_validation_loss[:primary_patience]):\n","                    break\n","            \n","        print('End of PDNN training.')\n","        print(\"Training time: %s seconds\" % (time.time() - start_time))\n","        sess.close()\n","        return self\n","    \n","    def transform(self, X, y=None):\n","        return self\n","    \n","    def predict(self, X, y=None):\n","        with tf.Session(graph=tf.Graph()) as sess:\n","            tf.saved_model.loader.load(sess, [\"tag\"], r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary'+str(self.name))\n","            graph = tf.get_default_graph()\n","            primary_inputs = graph.get_tensor_by_name(\"primary_inputs:0\")\n","            primary_predictions = graph.get_tensor_by_name(\"primary_predictions:0\")\n","            return sess.run([primary_predictions], feed_dict={primary_inputs:X})[0]\n","        \n","    def score(self, X, y):\n","        with tf.Session(graph=tf.Graph()) as sess:\n","            tf.saved_model.loader.load(sess, [\"tag\"], r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary'+str(self.name))\n","            graph = tf.get_default_graph()\n","            primary_inputs = graph.get_tensor_by_name(\"primary_inputs:0\")\n","            primary_predictions = graph.get_tensor_by_name(\"primary_predictions:0\")\n","            return print('R^2: {}'.format(r2_score(y,pd.DataFrame(sess.run([primary_predictions],\n","                                             feed_dict={primary_inputs:X})).transpose())))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Ch5a0oe6INt","colab_type":"code","colab":{}},"source":["# build a class to combine the base predictions for the meta DNN\n","\n","class Meta_Features(TransformerMixin,BaseEstimator):\n","    \n","    feats = named_features\n","    \n","    lr_meta = GridSearchCV(Ridge(),\n","                                param_grid = {'alpha':[1]},\n","                                cv=2, n_jobs = -1, refit = True)\n","\n","    rf_meta = GridSearchCV(RandomForestRegressor(),\n","                                param_grid = {'n_estimators':[2]},\n","                                cv=2, n_jobs = -1, refit = True)\n","\n","    xgb_meta = GridSearchCV(xgb.XGBRegressor(),\n","                                    param_grid = {'n_estimators':[2], 'max_depth': [2], 'alpha': [1], 'objective': ['reg:squarederror']},\n","                                    cv=2, n_jobs = -1, refit = True)\n","\n","    PDNN_number = 5\n","    PDNN_Networks = []\n","    for network in range(PDNN_number):\n","        PDNN_Networks.append(Primary_DNN(name = len(PDNN_Networks)+1))\n","    \n","    def __init__(self):\n","        pass\n","    \n","    def transform(self, X):\n","        \n","        X1 = X.copy()\n","        X1.set_index('ID Number', inplace = True)\n","        X, y = X1.drop(['Market 1 Sold (vol::date)'],axis=1), X1.loc[:,'Market 1 Sold (vol::date)']\n","        \n","        meta_df = pd.DataFrame(self.feats.transform(X)).set_index(y.index)\n","    \n","        PDNN_Predictions = []\n","        for network in self.PDNN_Networks:\n","            PDNN_Predictions.append(pd.Series(network.predict(meta_df),\n","                                              name = 'PDNN'+str(self.PDNN_Networks.index(network)+1), index = meta_df.index))\n","\n","        PDNN_Predictions = pd.concat(PDNN_Predictions, axis = 1)\n","        \n","        X, y, z = pd.concat([meta_df,\n","                             PDNN_Predictions,\n","                          pd.Series(self.lr_meta.predict(meta_df), name = 'LR', index = meta_df.index),\n","                          pd.Series(self.rf_meta.predict(meta_df), name = 'RF', index = meta_df.index),\n","                          pd.Series(self.xgb_meta.predict(meta_df), name = 'XGB', index = meta_df.index)],\n","                            axis = 1, join='inner'),y, self.PDNN_number\n","        \n","        return X, y, z\n","    \n","    def fit_transform(self, X, y):\n","        \n","        X1 = X.copy()\n","        X1.set_index('ID Number', inplace = True)\n","        X, y = X1.drop(['Market 1 Sold (vol::date)'],axis=1), X1.loc[:,'Market 1 Sold (vol::date)']\n","        \n","        x_train_meta, x_test_meta, y_train_meta, y_test_meta = train_test_split(X, y, train_size = 0.8, \n","                                                                            test_size = 0.2, random_state = 7)\n","\n","        # cut outliers and log-transform the targets\n","        \n","#        z_score = np.abs(stats.zscore(y_train_meta))\n","#        resultant_df = big_df[(big_df.index not in small_df.index)]\n","#        x_train_pruned = x_train_meta.loc[y_train_pruned.index]\n","\n","        y_train_log = np.log(y_train_meta)\n","        y_test_log = np.log(y_test_meta)\n","\n","        self.feats.fit(x_train_meta, y_train_log)\n","        train_df = pd.DataFrame(self.feats.transform(x_train_meta)).set_index(y_train_log.index)\n","                        \n","        self.lr_meta.fit(train_df, y_train_log)\n","        self.rf_meta.fit(train_df, y_train_log)\n","        self.xgb_meta.fit(train_df, y_train_log)\n","        for network in self.PDNN_Networks:\n","            network.fit(train_df, y_train_log)\n","        \n","        meta_df = pd.DataFrame(self.feats.transform(x_test_meta)).set_index(y_test_log.index)\n","        \n","        PDNN_Predictions = []\n","        for network in self.PDNN_Networks:\n","            PDNN_Predictions.append(pd.Series(network.predict(meta_df),\n","                                              name = 'PDNN'+str(self.PDNN_Networks.index(network)+1), index = meta_df.index))\n","\n","        PDNN_Predictions = pd.concat(PDNN_Predictions, axis = 1)\n","        \n","        X, y, z = pd.concat([meta_df,\n","                             PDNN_Predictions,\n","                          pd.Series(self.lr_meta.predict(meta_df), name = 'LR', index = meta_df.index),\n","                          pd.Series(self.rf_meta.predict(meta_df), name = 'RF', index = meta_df.index),\n","                          pd.Series(self.xgb_meta.predict(meta_df), name = 'XGB', index = meta_df.index)],\n","                            axis = 1, join='inner'),y_test_log, self.PDNN_number\n","        \n","        return X, y, z"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d74s9f9h6INw","colab_type":"code","colab":{}},"source":["#create a class fit, test and predict with the meta DNN model\n","\n","class Meta_DNN(TransformerMixin,BaseEstimator):\n","    def __init__(self):\n","        pass\n","    def fit(self, X, y = None):\n","        \n","        X, y, z = X[0], X[1], X[2]\n","        \n","        #split the meta data to train and validation\n","\n","        x_train_meta, x_val_meta, y_train_meta, y_val_meta = train_test_split(X, y,\n","                                                                             train_size = 0.8, test_size = 0.2, random_state = 7)\n","        \n","        np.savez('SP_data_train_meta',inputs = x_train_meta,targets = y_train_meta)\n","        np.savez('SP_data_validate_meta',inputs = x_val_meta,targets = y_val_meta)\n","        \n","        # set the parameters for the meta DNN model\n","        \n","        output_size = 1\n","        input_size = x_train_meta.shape[1]\n","        \n","        hidden_layer_size = 512\n","\n","        tf.reset_default_graph()\n","        \n","        inputs = tf.placeholder(tf.float32,[None, input_size], name = 'inputs')\n","        targets = tf.placeholder(tf.float32,[None])\n","\n","        weights_1 = tf.get_variable(\"weights_1\",[input_size, hidden_layer_size])\n","        biases_1 = tf.get_variable(\"biases_1\",[hidden_layer_size])\n","\n","        drop_1 = tf.nn.dropout(inputs, rate = 0.01)\n","        outputs_1 = tf.nn.relu(tf.matmul(drop_1,weights_1)+biases_1)\n","\n","        weights_2 = tf.get_variable(\"weights_2\",[hidden_layer_size, hidden_layer_size/2])\n","        biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size/2])\n","\n","        drop_2 = tf.nn.dropout(outputs_1, rate = 0.01)\n","        outputs_2 = tf.nn.relu(tf.matmul(drop_2,weights_2)+biases_2)\n","\n","        weights_3 = tf.get_variable(\"weights_3\",[hidden_layer_size/2, hidden_layer_size/4])\n","        biases_3 = tf.get_variable(\"biases_3\",[hidden_layer_size/4])\n","\n","        drop_3 = tf.nn.dropout(outputs_2, rate = 0.01)\n","        outputs_3 = tf.nn.relu(tf.matmul(drop_3,weights_3)+biases_3)\n","\n","        weights_4 = tf.get_variable(\"weights_4\",[hidden_layer_size/4, hidden_layer_size/8])\n","        biases_4 = tf.get_variable(\"biases_4\",[hidden_layer_size/8])\n","\n","        drop_4 = tf.nn.dropout(outputs_3, rate = 0.01)\n","        outputs_4 = tf.nn.relu(tf.matmul(drop_4,weights_4)+biases_4)\n","\n","        weights_5 = tf.get_variable(\"weights_5\",[hidden_layer_size/8, output_size])\n","        biases_5 = tf.get_variable(\"biases_5\",[output_size])\n","\n","        outputs = tf.matmul(outputs_4,weights_5)+biases_5\n","\n","        #set the parameters for the meta DNN model\n","        #reduce_sum at final output to 'collapse' vector to single value\n","\n","        predictions = tf.math.reduce_sum(outputs, axis = 1, name = 'predictions')\n","        loss = tf.losses.mean_squared_error(predictions = predictions, labels = targets)\n","        mean_loss = tf.reduce_mean(loss)\n","        optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n","        accuracy = tf.reduce_mean(abs(predictions - targets))\n","        sess_meta = tf.InteractiveSession()\n","        initializer = tf.global_variables_initializer()\n","        sess_meta.run(initializer)\n","\n","        batch_size = 16\n","        max_epochs = 50\n","\n","        train_data = SP_Data_Reader('train_meta',batch_size)\n","        validation_data = SP_Data_Reader('validate_meta')\n","\n","        agg_validation_loss = [9999999.]\n","        meta_patience = -4\n","\n","        #train the meta DNN model\n","        start_time = time.time()\n","        for epoch_counter in range(max_epochs):\n","            curr_epoch_loss = 0.\n","            for inputs_batch, targets_batch in train_data:\n","                _, batch_loss = sess_meta.run([optimize, mean_loss],feed_dict={inputs: inputs_batch,targets: targets_batch})\n","                curr_epoch_loss = curr_epoch_loss + batch_loss\n","            curr_epoch_loss = curr_epoch_loss/train_data.batch_count\n","\n","            validation_loss = 0.\n","            validation_accuracy = 0.\n","\n","            for inputs_batch, targets_batch in validation_data:\n","                validation_loss, validation_accuracy = sess_meta.run([mean_loss,accuracy],feed_dict={inputs: inputs_batch,targets: targets_batch})\n","\n","            print('Meta Epoch '+str(epoch_counter+1)+\n","                  '. Training loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n","                  '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n","                  '. Validation accuracy abs: '+'{0:.2f}'.format(validation_accuracy)+'m')\n","\n","            #set the early stop mechanism and saving the best-performing model\n","\n","\n","            if validation_loss < min(agg_validation_loss):\n","                if os.path.isdir(r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta'):\n","                    shutil.rmtree(r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta')\n","\n","                meta_builder = tf.saved_model.builder.SavedModelBuilder \\\n","                (r'/content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta')\n","                meta_builder.add_meta_graph_and_variables(sess_meta,[\"tag\"],\n","                                                          signature_def_map={\"meta_model\": tf.saved_model.signature_def_utils.predict_signature_def(\n","                            inputs= {\"inputs\": inputs},\n","                            outputs= {\"predictions\": predictions})})\n","                meta_builder.save()\n","\n","            agg_validation_loss.append(validation_loss)\n","\n","            if epoch_counter > abs(meta_patience):\n","                if min(agg_validation_loss[meta_patience:]) > min(agg_validation_loss[:meta_patience]):\n","                    break\n","\n","        print('End of MDNN training.')\n","        print(\"Training time: %s seconds\" % (time.time() - start_time))\n","        sess_meta.close()\n","        return self\n","    \n","    def predict(self, X, y = None):\n","        X = X[0]\n","        with tf.Session(graph=tf.Graph()) as sess:\n","            tf.saved_model.loader.load(sess, [\"tag\"], r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta')\n","            graph = tf.get_default_graph()\n","            inputs = graph.get_tensor_by_name(\"inputs:0\")\n","            predictions = graph.get_tensor_by_name(\"predictions:0\")\n","            return np.exp(sess.run([predictions], feed_dict={inputs:X})[0])\n","        \n","    def score(self, X, y = None):\n","        X, y, z = X[0], X[1], X[2]\n","        \n","        with tf.Session(graph=tf.Graph()) as sess:\n","            tf.saved_model.loader.load(sess, [\"tag\"], r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta')\n","            graph = tf.get_default_graph()\n","            inputs = graph.get_tensor_by_name(\"inputs:0\")\n","            predictions = graph.get_tensor_by_name(\"predictions:0\")\n","            result = pd.concat([X.merge(y.to_frame(),how = 'left',left_index = True,\n","                                     right_index = True),\n","                             pd.DataFrame(np.exp(sess.run([predictions],\n","                                             feed_dict={inputs:X}))).transpose().set_index(X.index)], axis = 1)\n","        \n","        for model in range(z+3):\n","            result.iloc[:,(model+3)*-1] = np.exp(result.iloc[:,(model+3)*-1])\n","        \n","        result.to_csv('./drive/My Drive/Colab Notebooks/Regression/Ensemble_Test_Results.csv')\n","        \n","        Model_name = []\n","        Abs_dev_score = []\n","        RMSLE_score = []\n","        R2_score = []\n","        \n","        for model in range(z+3):\n","            Model_name.append(result.columns[(model+3)*-1])\n","        \n","        for model in range(z+3):\n","            Abs_dev_score.append(round(sum(abs(result.iloc[:,-2]-result.iloc[:,(model+3)*-1]))/len(result.iloc[:,-2]),2))\n","\n","        for model in range(z+3):\n","            RMSLE_score.append(round(sqrt(mean_squared_error(np.log(result.iloc[:,-2]),np.log(result.iloc[:,(model+3)*-1]))),2))\n","\n","        for model in range(z+3):\n","            R2_score.append(round(r2_score(result.iloc[:,-2],result.iloc[:,(model+3)*-1]),2))\n","\n","        Scores = pd.DataFrame(data = [round(sum(abs(result.iloc[:,-2]-result.iloc[:,-1]))/len(result.iloc[:,-2]),2),\n","                                      round(sqrt(mean_squared_error(np.log(result.iloc[:,-2]),np.log(result.iloc[:,-1]))),2),\n","                                      round(r2_score(result.iloc[:,-2],result.iloc[:,-1]),2)],\n","                                    columns = ['Meta DNN'],index = ['Absolute Deviation','RMSLE','R^2'])\n","        \n","        scores_data = pd.DataFrame(data = [Abs_dev_score, RMSLE_score, R2_score], columns = Model_name,\n","                                   index = ['Absolute Deviation','RMSLE','R^2'])\n","        \n","        Scores = pd.concat([scores_data, Scores],axis = 1)\n","        \n","        return Scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gbfu_ESF6INz","colab_type":"code","colab":{}},"source":["#define the steps in the meta pipelines\n","\n","meta_ppl = Pipeline([('meta_feats',Meta_Features()),\n","                     ('MDNN',Meta_DNN())])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"hSOqTk9E6IN1","colab_type":"code","outputId":"d377e487-1531-4eb0-e1a5-ea10c38d0053","executionInfo":{"status":"ok","timestamp":1570699432206,"user_tz":-180,"elapsed":309535,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#fit the pipeline with the train set\n","\n","meta_ppl.fit(df_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n","  if getattr(data, 'base', None) is not None and \\\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Primary DNN1, Epoch: 1. Training loss: 1.968. Validation loss: 1.766. Validation accuracy abs: 1.06m\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 2. Training loss: 1.713. Validation loss: 1.698. Validation accuracy abs: 1.04m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 3. Training loss: 1.660. Validation loss: 1.686. Validation accuracy abs: 1.03m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 4. Training loss: 1.618. Validation loss: 1.646. Validation accuracy abs: 1.02m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 5. Training loss: 1.599. Validation loss: 1.619. Validation accuracy abs: 1.01m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 6. Training loss: 1.580. Validation loss: 1.601. Validation accuracy abs: 1.01m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 7. Training loss: 1.569. Validation loss: 1.585. Validation accuracy abs: 1.00m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 8. Training loss: 1.544. Validation loss: 1.585. Validation accuracy abs: 1.00m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 9. Training loss: 1.537. Validation loss: 1.550. Validation accuracy abs: 0.99m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 10. Training loss: 1.529. Validation loss: 1.564. Validation accuracy abs: 0.99m\n","Primary DNN1, Epoch: 11. Training loss: 1.517. Validation loss: 1.549. Validation accuracy abs: 0.99m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 12. Training loss: 1.510. Validation loss: 1.540. Validation accuracy abs: 0.99m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 13. Training loss: 1.501. Validation loss: 1.537. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 14. Training loss: 1.494. Validation loss: 1.521. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 15. Training loss: 1.480. Validation loss: 1.524. Validation accuracy abs: 0.98m\n","Primary DNN1, Epoch: 16. Training loss: 1.484. Validation loss: 1.521. Validation accuracy abs: 0.98m\n","Primary DNN1, Epoch: 17. Training loss: 1.477. Validation loss: 1.522. Validation accuracy abs: 0.98m\n","Primary DNN1, Epoch: 18. Training loss: 1.463. Validation loss: 1.514. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 19. Training loss: 1.466. Validation loss: 1.507. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 20. Training loss: 1.474. Validation loss: 1.503. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 21. Training loss: 1.450. Validation loss: 1.497. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 22. Training loss: 1.451. Validation loss: 1.487. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 23. Training loss: 1.439. Validation loss: 1.500. Validation accuracy abs: 0.97m\n","Primary DNN1, Epoch: 24. Training loss: 1.437. Validation loss: 1.488. Validation accuracy abs: 0.97m\n","Primary DNN1, Epoch: 25. Training loss: 1.421. Validation loss: 1.480. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 26. Training loss: 1.430. Validation loss: 1.462. Validation accuracy abs: 0.95m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 27. Training loss: 1.430. Validation loss: 1.492. Validation accuracy abs: 0.96m\n","Primary DNN1, Epoch: 28. Training loss: 1.415. Validation loss: 1.458. Validation accuracy abs: 0.95m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 29. Training loss: 1.410. Validation loss: 1.482. Validation accuracy abs: 0.96m\n","Primary DNN1, Epoch: 30. Training loss: 1.402. Validation loss: 1.462. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 31. Training loss: 1.392. Validation loss: 1.467. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 32. Training loss: 1.402. Validation loss: 1.457. Validation accuracy abs: 0.95m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 33. Training loss: 1.380. Validation loss: 1.465. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 34. Training loss: 1.385. Validation loss: 1.433. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 35. Training loss: 1.371. Validation loss: 1.430. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 36. Training loss: 1.377. Validation loss: 1.439. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 37. Training loss: 1.361. Validation loss: 1.432. Validation accuracy abs: 0.94m\n","Primary DNN1, Epoch: 38. Training loss: 1.357. Validation loss: 1.426. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 39. Training loss: 1.365. Validation loss: 1.447. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 40. Training loss: 1.363. Validation loss: 1.429. Validation accuracy abs: 0.94m\n","Primary DNN1, Epoch: 41. Training loss: 1.350. Validation loss: 1.450. Validation accuracy abs: 0.95m\n","Primary DNN1, Epoch: 42. Training loss: 1.351. Validation loss: 1.418. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 43. Training loss: 1.352. Validation loss: 1.411. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/saved_model.pb\n","Primary DNN1, Epoch: 44. Training loss: 1.348. Validation loss: 1.423. Validation accuracy abs: 0.94m\n","Primary DNN1, Epoch: 45. Training loss: 1.339. Validation loss: 1.435. Validation accuracy abs: 0.94m\n","Primary DNN1, Epoch: 46. Training loss: 1.342. Validation loss: 1.417. Validation accuracy abs: 0.93m\n","Primary DNN1, Epoch: 47. Training loss: 1.333. Validation loss: 1.425. Validation accuracy abs: 0.94m\n","End of PDNN training.\n","Training time: 61.968761682510376 seconds\n","Primary DNN2, Epoch: 1. Training loss: 1.773. Validation loss: 1.636. Validation accuracy abs: 1.02m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 2. Training loss: 1.599. Validation loss: 1.585. Validation accuracy abs: 1.00m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 3. Training loss: 1.554. Validation loss: 1.542. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 4. Training loss: 1.514. Validation loss: 1.510. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 5. Training loss: 1.488. Validation loss: 1.489. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 6. Training loss: 1.478. Validation loss: 1.473. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 7. Training loss: 1.428. Validation loss: 1.456. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 8. Training loss: 1.405. Validation loss: 1.468. Validation accuracy abs: 0.95m\n","Primary DNN2, Epoch: 9. Training loss: 1.392. Validation loss: 1.439. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 10. Training loss: 1.373. Validation loss: 1.424. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 11. Training loss: 1.352. Validation loss: 1.419. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 12. Training loss: 1.341. Validation loss: 1.383. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 13. Training loss: 1.326. Validation loss: 1.387. Validation accuracy abs: 0.92m\n","Primary DNN2, Epoch: 14. Training loss: 1.317. Validation loss: 1.365. Validation accuracy abs: 0.91m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 15. Training loss: 1.305. Validation loss: 1.374. Validation accuracy abs: 0.92m\n","Primary DNN2, Epoch: 16. Training loss: 1.300. Validation loss: 1.354. Validation accuracy abs: 0.91m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 17. Training loss: 1.291. Validation loss: 1.339. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 18. Training loss: 1.283. Validation loss: 1.336. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 19. Training loss: 1.277. Validation loss: 1.336. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 20. Training loss: 1.263. Validation loss: 1.347. Validation accuracy abs: 0.91m\n","Primary DNN2, Epoch: 21. Training loss: 1.260. Validation loss: 1.317. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/saved_model.pb\n","Primary DNN2, Epoch: 22. Training loss: 1.255. Validation loss: 1.326. Validation accuracy abs: 0.90m\n","Primary DNN2, Epoch: 23. Training loss: 1.254. Validation loss: 1.321. Validation accuracy abs: 0.89m\n","Primary DNN2, Epoch: 24. Training loss: 1.251. Validation loss: 1.331. Validation accuracy abs: 0.90m\n","Primary DNN2, Epoch: 25. Training loss: 1.237. Validation loss: 1.325. Validation accuracy abs: 0.90m\n","End of PDNN training.\n","Training time: 52.73528480529785 seconds\n","Primary DNN3, Epoch: 1. Training loss: 2.712. Validation loss: 1.552. Validation accuracy abs: 0.99m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 2. Training loss: 1.524. Validation loss: 1.498. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 3. Training loss: 1.469. Validation loss: 1.449. Validation accuracy abs: 0.95m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 4. Training loss: 1.427. Validation loss: 1.427. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 5. Training loss: 1.384. Validation loss: 1.399. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 6. Training loss: 1.350. Validation loss: 1.379. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 7. Training loss: 1.342. Validation loss: 1.358. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/saved_model.pb\n","Primary DNN3, Epoch: 8. Training loss: 1.335. Validation loss: 1.362. Validation accuracy abs: 0.91m\n","Primary DNN3, Epoch: 9. Training loss: 1.314. Validation loss: 1.366. Validation accuracy abs: 0.92m\n","Primary DNN3, Epoch: 10. Training loss: 1.316. Validation loss: 1.369. Validation accuracy abs: 0.92m\n","Primary DNN3, Epoch: 11. Training loss: 1.306. Validation loss: 1.380. Validation accuracy abs: 0.92m\n","End of PDNN training.\n","Training time: 17.370994806289673 seconds\n","Primary DNN4, Epoch: 1. Training loss: 5.171. Validation loss: 1.608. Validation accuracy abs: 1.01m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 2. Training loss: 1.543. Validation loss: 1.524. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 3. Training loss: 1.502. Validation loss: 1.487. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 4. Training loss: 1.469. Validation loss: 1.505. Validation accuracy abs: 0.97m\n","Primary DNN4, Epoch: 5. Training loss: 1.459. Validation loss: 1.464. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 6. Training loss: 1.430. Validation loss: 1.431. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 7. Training loss: 1.418. Validation loss: 1.420. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 8. Training loss: 1.403. Validation loss: 1.426. Validation accuracy abs: 0.94m\n","Primary DNN4, Epoch: 9. Training loss: 1.403. Validation loss: 1.424. Validation accuracy abs: 0.94m\n","Primary DNN4, Epoch: 10. Training loss: 1.385. Validation loss: 1.391. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 11. Training loss: 1.362. Validation loss: 1.375. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 12. Training loss: 1.356. Validation loss: 1.375. Validation accuracy abs: 0.92m\n","Primary DNN4, Epoch: 13. Training loss: 1.351. Validation loss: 1.427. Validation accuracy abs: 0.93m\n","Primary DNN4, Epoch: 14. Training loss: 1.347. Validation loss: 1.363. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 15. Training loss: 1.338. Validation loss: 1.366. Validation accuracy abs: 0.92m\n","Primary DNN4, Epoch: 16. Training loss: 1.332. Validation loss: 1.360. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 17. Training loss: 1.323. Validation loss: 1.408. Validation accuracy abs: 0.93m\n","Primary DNN4, Epoch: 18. Training loss: 1.323. Validation loss: 1.356. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 19. Training loss: 1.302. Validation loss: 1.349. Validation accuracy abs: 0.91m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 20. Training loss: 1.303. Validation loss: 1.360. Validation accuracy abs: 0.91m\n","Primary DNN4, Epoch: 21. Training loss: 1.293. Validation loss: 1.334. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/saved_model.pb\n","Primary DNN4, Epoch: 22. Training loss: 1.279. Validation loss: 1.347. Validation accuracy abs: 0.91m\n","Primary DNN4, Epoch: 23. Training loss: 1.285. Validation loss: 1.345. Validation accuracy abs: 0.90m\n","Primary DNN4, Epoch: 24. Training loss: 1.281. Validation loss: 1.344. Validation accuracy abs: 0.90m\n","Primary DNN4, Epoch: 25. Training loss: 1.276. Validation loss: 1.346. Validation accuracy abs: 0.90m\n","End of PDNN training.\n","Training time: 37.050973653793335 seconds\n","Primary DNN5, Epoch: 1. Training loss: 1.812. Validation loss: 1.686. Validation accuracy abs: 1.03m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 2. Training loss: 1.604. Validation loss: 1.585. Validation accuracy abs: 1.00m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 3. Training loss: 1.554. Validation loss: 1.535. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 4. Training loss: 1.512. Validation loss: 1.532. Validation accuracy abs: 0.98m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 5. Training loss: 1.484. Validation loss: 1.508. Validation accuracy abs: 0.97m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 6. Training loss: 1.443. Validation loss: 1.486. Validation accuracy abs: 0.96m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 7. Training loss: 1.426. Validation loss: 1.443. Validation accuracy abs: 0.95m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 8. Training loss: 1.405. Validation loss: 1.430. Validation accuracy abs: 0.94m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 9. Training loss: 1.395. Validation loss: 1.434. Validation accuracy abs: 0.94m\n","Primary DNN5, Epoch: 10. Training loss: 1.372. Validation loss: 1.412. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 11. Training loss: 1.355. Validation loss: 1.404. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 12. Training loss: 1.339. Validation loss: 1.388. Validation accuracy abs: 0.93m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 13. Training loss: 1.334. Validation loss: 1.387. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 14. Training loss: 1.315. Validation loss: 1.377. Validation accuracy abs: 0.92m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 15. Training loss: 1.301. Validation loss: 1.380. Validation accuracy abs: 0.92m\n","Primary DNN5, Epoch: 16. Training loss: 1.291. Validation loss: 1.362. Validation accuracy abs: 0.91m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 17. Training loss: 1.284. Validation loss: 1.351. Validation accuracy abs: 0.91m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 18. Training loss: 1.277. Validation loss: 1.336. Validation accuracy abs: 0.90m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 19. Training loss: 1.264. Validation loss: 1.337. Validation accuracy abs: 0.90m\n","Primary DNN5, Epoch: 20. Training loss: 1.266. Validation loss: 1.344. Validation accuracy abs: 0.91m\n","Primary DNN5, Epoch: 21. Training loss: 1.252. Validation loss: 1.348. Validation accuracy abs: 0.91m\n","Primary DNN5, Epoch: 22. Training loss: 1.249. Validation loss: 1.310. Validation accuracy abs: 0.89m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 23. Training loss: 1.239. Validation loss: 1.325. Validation accuracy abs: 0.90m\n","Primary DNN5, Epoch: 24. Training loss: 1.224. Validation loss: 1.331. Validation accuracy abs: 0.90m\n","Primary DNN5, Epoch: 25. Training loss: 1.219. Validation loss: 1.305. Validation accuracy abs: 0.89m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/saved_model.pb\n","Primary DNN5, Epoch: 26. Training loss: 1.225. Validation loss: 1.316. Validation accuracy abs: 0.89m\n","Primary DNN5, Epoch: 27. Training loss: 1.215. Validation loss: 1.331. Validation accuracy abs: 0.90m\n","Primary DNN5, Epoch: 28. Training loss: 1.214. Validation loss: 1.319. Validation accuracy abs: 0.89m\n","Primary DNN5, Epoch: 29. Training loss: 1.198. Validation loss: 1.306. Validation accuracy abs: 0.89m\n","End of PDNN training.\n","Training time: 50.757630586624146 seconds\n","WARNING:tensorflow:From <ipython-input-13-a282f3040980>:139: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/variables/variables\n","Meta Epoch 1. Training loss: 1.064. Validation loss: 1.056. Validation accuracy abs: 0.77m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/saved_model.pb\n","Meta Epoch 2. Training loss: 1.017. Validation loss: 1.043. Validation accuracy abs: 0.77m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/saved_model.pb\n","Meta Epoch 3. Training loss: 1.003. Validation loss: 1.030. Validation accuracy abs: 0.76m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/saved_model.pb\n","Meta Epoch 4. Training loss: 0.987. Validation loss: 1.030. Validation accuracy abs: 0.76m\n","Meta Epoch 5. Training loss: 0.981. Validation loss: 1.017. Validation accuracy abs: 0.76m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/saved_model.pb\n","Meta Epoch 6. Training loss: 0.968. Validation loss: 1.016. Validation accuracy abs: 0.76m\n","INFO:tensorflow:No assets to save.\n","INFO:tensorflow:No assets to write.\n","INFO:tensorflow:SavedModel written to: /content/drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/saved_model.pb\n","Meta Epoch 7. Training loss: 0.962. Validation loss: 1.021. Validation accuracy abs: 0.77m\n","Meta Epoch 8. Training loss: 0.952. Validation loss: 1.018. Validation accuracy abs: 0.76m\n","Meta Epoch 9. Training loss: 0.936. Validation loss: 1.025. Validation accuracy abs: 0.77m\n","Meta Epoch 10. Training loss: 0.918. Validation loss: 1.034. Validation accuracy abs: 0.77m\n","End of MDNN training.\n","Training time: 15.948937177658081 seconds\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('meta_feats', Meta_Features()), ('MDNN', Meta_DNN())],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AHzAUTno6IN7","colab_type":"code","outputId":"adcdcdd8-8277-4697-9c95-4bb01bcf4560","executionInfo":{"status":"ok","timestamp":1570699436541,"user_tz":-180,"elapsed":313864,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":346}},"source":["#score the pipeline with the test set\n","\n","meta_ppl.score(df_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/variables/variables\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>XGB</th>\n","      <th>RF</th>\n","      <th>LR</th>\n","      <th>PDNN5</th>\n","      <th>PDNN4</th>\n","      <th>PDNN3</th>\n","      <th>PDNN2</th>\n","      <th>PDNN1</th>\n","      <th>Meta DNN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Absolute Deviation</th>\n","      <td>3.27</td>\n","      <td>2.81</td>\n","      <td>3.03</td>\n","      <td>2.82</td>\n","      <td>2.89</td>\n","      <td>2.88</td>\n","      <td>2.87</td>\n","      <td>2.92</td>\n","      <td>2.61</td>\n","    </tr>\n","    <tr>\n","      <th>RMSLE</th>\n","      <td>1.40</td>\n","      <td>1.12</td>\n","      <td>1.24</td>\n","      <td>1.13</td>\n","      <td>1.14</td>\n","      <td>1.15</td>\n","      <td>1.14</td>\n","      <td>1.17</td>\n","      <td>1.01</td>\n","    </tr>\n","    <tr>\n","      <th>R^2</th>\n","      <td>-0.05</td>\n","      <td>0.22</td>\n","      <td>0.06</td>\n","      <td>0.17</td>\n","      <td>0.09</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.11</td>\n","      <td>0.27</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     XGB    RF    LR  PDNN5  ...  PDNN3  PDNN2  PDNN1  Meta DNN\n","Absolute Deviation  3.27  2.81  3.03   2.82  ...   2.88   2.87   2.92      2.61\n","RMSLE               1.40  1.12  1.24   1.13  ...   1.15   1.14   1.17      1.01\n","R^2                -0.05  0.22  0.06   0.17  ...   0.12   0.13   0.11      0.27\n","\n","[3 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"nKMyYRQ66IN_","colab_type":"code","outputId":"99494006-3bdd-4b49-bdf5-d1fd0503ad5b","executionInfo":{"status":"ok","timestamp":1570699440174,"user_tz":-180,"elapsed":317487,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":530}},"source":["#import the new data for inference via the pipeline\n","\n","df_new = pd.read_excel('./drive/My Drive/Colab Notebooks/Regression/Pipeline_Test.xlsx', encoding='latin-1')\n","df_new.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID Number</th>\n","      <th>Market 1 Sold (vol::date)</th>\n","      <th>Market 1 Country</th>\n","      <th>Bond Providers</th>\n","      <th>Market 1 Wrapper</th>\n","      <th>Market 1 Currency</th>\n","      <th>Product Type</th>\n","      <th>Strike Levels (underlying</th>\n","      <th>Payoff Type</th>\n","      <th>Payoff Group</th>\n","      <th>Asset Class(omitted)</th>\n","      <th>Income Type</th>\n","      <th>Date</th>\n","      <th>Term</th>\n","      <th>Headline Rate</th>\n","      <th>Minimum Return: Lower</th>\n","      <th>Sales Commission</th>\n","      <th>Participation Rate</th>\n","      <th>Barrier 1</th>\n","      <th>Digital Coupon</th>\n","      <th>Base Interest Rate</th>\n","      <th>Deposit Rate</th>\n","      <th>Implied Volatility</th>\n","      <th>Most Recent GDP growth</th>\n","      <th>PMM vs T3MA</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>68</td>\n","      <td>2.863440</td>\n","      <td>asdf</td>\n","      <td>Bank of Icecream</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>Share Basket (Unspecified)</td>\n","      <td>Callable, Uncapped Call</td>\n","      <td>Exotic</td>\n","      <td>Equity (Share Basket)</td>\n","      <td>NaN</td>\n","      <td>2015-03-11</td>\n","      <td>9.008219</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>16.870001</td>\n","      <td>-0.544198</td>\n","      <td>0.791069</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>117</td>\n","      <td>2.882887</td>\n","      <td>Canada</td>\n","      <td>gfdh</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>iShares S&amp;P/TSX Capped Energy Index ETF</td>\n","      <td>Knock Out, Protected Tracker</td>\n","      <td>Participation</td>\n","      <td>Equity (Single Index)</td>\n","      <td>NaN</td>\n","      <td>2015-04-03</td>\n","      <td>3.008219</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>-30.0</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>13.670000</td>\n","      <td>-0.269524</td>\n","      <td>0.988702</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>132</td>\n","      <td>3.803878</td>\n","      <td>Canada</td>\n","      <td>Bank of Muffins</td>\n","      <td>hgfj</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>NaN</td>\n","      <td>Capped Call, Putable</td>\n","      <td>Exotic</td>\n","      <td>Equity (Share Basket)</td>\n","      <td>NaN</td>\n","      <td>2015-06-24</td>\n","      <td>5.005479</td>\n","      <td>NaN</td>\n","      <td>102.5</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>13.190000</td>\n","      <td>-0.269524</td>\n","      <td>1.130888</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>210</td>\n","      <td>4.418104</td>\n","      <td>Canada</td>\n","      <td>Bank of Chocolate</td>\n","      <td>Note</td>\n","      <td>jhgk</td>\n","      <td>Growth</td>\n","      <td>Eurostoxx 50</td>\n","      <td>Knock Out, Protected Tracker</td>\n","      <td>Capital Protection</td>\n","      <td>Equity (Single Index)</td>\n","      <td>NaN</td>\n","      <td>2015-07-13</td>\n","      <td>5.005479</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>-25.0</td>\n","      <td>NaN</td>\n","      <td>0.75</td>\n","      <td>1.25</td>\n","      <td>16.790001</td>\n","      <td>0.350715</td>\n","      <td>0.927680</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>227</td>\n","      <td>4.505167</td>\n","      <td>Canada</td>\n","      <td>Bank of Icecream</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>kjhgk</td>\n","      <td>Eurostoxx 50</td>\n","      <td>Digital, Knock Out, Protected Tracker</td>\n","      <td>Participation</td>\n","      <td>Equity (Single Index)</td>\n","      <td>Variable</td>\n","      <td>2015-07-05</td>\n","      <td>6.030137</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>-25.0</td>\n","      <td>3.075</td>\n","      <td>0.75</td>\n","      <td>1.25</td>\n","      <td>17.010000</td>\n","      <td>0.350715</td>\n","      <td>0.940003</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID Number  Market 1 Sold (vol::date)  ... Most Recent GDP growth PMM vs T3MA\n","0         68                   2.863440  ...              -0.544198    0.791069\n","1        117                   2.882887  ...              -0.269524    0.988702\n","2        132                   3.803878  ...              -0.269524    1.130888\n","3        210                   4.418104  ...               0.350715    0.927680\n","4        227                   4.505167  ...               0.350715    0.940003\n","\n","[5 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xg25Z_ji6IOE","colab_type":"code","outputId":"8cec7e86-ea83-4ec6-aab8-7ee0d8a2074d","executionInfo":{"status":"ok","timestamp":1570699443362,"user_tz":-180,"elapsed":320670,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":751}},"source":["#execute the pipeline on the new data and extract new predictions with the file\n","\n","ppl_prediction = meta_ppl.predict(df_new)\n","New_pred = pd.Series(ppl_prediction,index = df_new.index)\n","New_est = df_new.merge(New_pred.to_frame(name = 'meta'),how = 'left',left_index = True, right_index = True)\n","New_est.to_csv('./drive/My Drive/Colab Notebooks/Regression/New_Preds.csv')\n","New_est.head()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary1/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary2/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary4/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary5/variables/variables\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Meta/variables/variables\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:131: RuntimeWarning: overflow encountered in exp\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID Number</th>\n","      <th>Market 1 Sold (vol::date)</th>\n","      <th>Market 1 Country</th>\n","      <th>Bond Providers</th>\n","      <th>Market 1 Wrapper</th>\n","      <th>Market 1 Currency</th>\n","      <th>Product Type</th>\n","      <th>Strike Levels (underlying</th>\n","      <th>Payoff Type</th>\n","      <th>Payoff Group</th>\n","      <th>Asset Class(omitted)</th>\n","      <th>Income Type</th>\n","      <th>Date</th>\n","      <th>Term</th>\n","      <th>Headline Rate</th>\n","      <th>Minimum Return: Lower</th>\n","      <th>Sales Commission</th>\n","      <th>Participation Rate</th>\n","      <th>Barrier 1</th>\n","      <th>Digital Coupon</th>\n","      <th>Base Interest Rate</th>\n","      <th>Deposit Rate</th>\n","      <th>Implied Volatility</th>\n","      <th>Most Recent GDP growth</th>\n","      <th>PMM vs T3MA</th>\n","      <th>meta</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>68</td>\n","      <td>2.863440</td>\n","      <td>asdf</td>\n","      <td>Bank of Icecream</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>Share Basket (Unspecified)</td>\n","      <td>Callable, Uncapped Call</td>\n","      <td>Exotic</td>\n","      <td>Equity (Share Basket)</td>\n","      <td>NaN</td>\n","      <td>2015-03-11</td>\n","      <td>9.008219</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>16.870001</td>\n","      <td>-0.544198</td>\n","      <td>0.791069</td>\n","      <td>2.067596</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>117</td>\n","      <td>2.882887</td>\n","      <td>Canada</td>\n","      <td>gfdh</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>iShares S&amp;P/TSX Capped Energy Index ETF</td>\n","      <td>Knock Out, Protected Tracker</td>\n","      <td>Participation</td>\n","      <td>Equity (Single Index)</td>\n","      <td>NaN</td>\n","      <td>2015-04-03</td>\n","      <td>3.008219</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>-30.0</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>13.670000</td>\n","      <td>-0.269524</td>\n","      <td>0.988702</td>\n","      <td>4.169425</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>132</td>\n","      <td>3.803878</td>\n","      <td>Canada</td>\n","      <td>Bank of Muffins</td>\n","      <td>hgfj</td>\n","      <td>CAD</td>\n","      <td>Growth</td>\n","      <td>NaN</td>\n","      <td>Capped Call, Putable</td>\n","      <td>Exotic</td>\n","      <td>Equity (Share Basket)</td>\n","      <td>NaN</td>\n","      <td>2015-06-24</td>\n","      <td>5.005479</td>\n","      <td>NaN</td>\n","      <td>102.5</td>\n","      <td>NaN</td>\n","      <td>100.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.00</td>\n","      <td>1.25</td>\n","      <td>13.190000</td>\n","      <td>-0.269524</td>\n","      <td>1.130888</td>\n","      <td>9.419992</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>210</td>\n","      <td>4.418104</td>\n","      <td>Canada</td>\n","      <td>Bank of Chocolate</td>\n","      <td>Note</td>\n","      <td>jhgk</td>\n","      <td>Growth</td>\n","      <td>Eurostoxx 50</td>\n","      <td>Knock Out, Protected Tracker</td>\n","      <td>Capital Protection</td>\n","      <td>Equity (Single Index)</td>\n","      <td>NaN</td>\n","      <td>2015-07-13</td>\n","      <td>5.005479</td>\n","      <td>NaN</td>\n","      <td>1.0</td>\n","      <td>2.5</td>\n","      <td>5.0</td>\n","      <td>-25.0</td>\n","      <td>NaN</td>\n","      <td>0.75</td>\n","      <td>1.25</td>\n","      <td>16.790001</td>\n","      <td>0.350715</td>\n","      <td>0.927680</td>\n","      <td>6.028340</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>227</td>\n","      <td>4.505167</td>\n","      <td>Canada</td>\n","      <td>Bank of Icecream</td>\n","      <td>Note</td>\n","      <td>CAD</td>\n","      <td>kjhgk</td>\n","      <td>Eurostoxx 50</td>\n","      <td>Digital, Knock Out, Protected Tracker</td>\n","      <td>Participation</td>\n","      <td>Equity (Single Index)</td>\n","      <td>Variable</td>\n","      <td>2015-07-05</td>\n","      <td>6.030137</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>NaN</td>\n","      <td>5.0</td>\n","      <td>-25.0</td>\n","      <td>3.075</td>\n","      <td>0.75</td>\n","      <td>1.25</td>\n","      <td>17.010000</td>\n","      <td>0.350715</td>\n","      <td>0.940003</td>\n","      <td>3.499247</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID Number  Market 1 Sold (vol::date)  ... PMM vs T3MA      meta\n","0         68                   2.863440  ...    0.791069  2.067596\n","1        117                   2.882887  ...    0.988702  4.169425\n","2        132                   3.803878  ...    1.130888  9.419992\n","3        210                   4.418104  ...    0.927680  6.028340\n","4        227                   4.505167  ...    0.940003  3.499247\n","\n","[5 rows x 26 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"6_tKq9M_2gOE","colab_type":"code","outputId":"0849f60c-7aaf-44ed-92ec-79b4117add1b","executionInfo":{"status":"ok","timestamp":1575624739644,"user_tz":-120,"elapsed":3951,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["with tf.Session(graph=tf.Graph()) as sess:\n","            tf.saved_model.loader.load(sess, [\"tag\"], r'./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3')\n","            graph = tf.get_default_graph()\n","            primary_inputs = graph.get_tensor_by_name(\"primary_inputs:0\")\n","            primary_predictions = graph.get_tensor_by_name(\"primary_predictions:0\")\n","            var = tf.trainable_variables()\n","            \n","var"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./drive/My Drive/Colab Notebooks/Regression/DNNs/Primary3/variables/variables\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<tf.Variable 'primary_weights_1:0' shape=(64, 1024) dtype=float32_ref>,\n"," <tf.Variable 'primary_biases_1:0' shape=(1024,) dtype=float32_ref>,\n"," <tf.Variable 'primary_weights_2:0' shape=(1024, 512) dtype=float32_ref>,\n"," <tf.Variable 'primary_biases_2:0' shape=(512,) dtype=float32_ref>,\n"," <tf.Variable 'primary_weights_3:0' shape=(512, 256) dtype=float32_ref>,\n"," <tf.Variable 'primary_biases_3:0' shape=(256,) dtype=float32_ref>,\n"," <tf.Variable 'primary_weights_4:0' shape=(256, 128) dtype=float32_ref>,\n"," <tf.Variable 'primary_biases_4:0' shape=(128,) dtype=float32_ref>,\n"," <tf.Variable 'primary_weights_5:0' shape=(128, 1) dtype=float32_ref>,\n"," <tf.Variable 'primary_biases_5:0' shape=(1,) dtype=float32_ref>]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"NUP_v3l62oUV","colab_type":"code","outputId":"fc9b101e-4c45-40dc-adc9-7af62ea741bc","executionInfo":{"status":"ok","timestamp":1575624610407,"user_tz":-120,"elapsed":2378,"user":{"displayName":"Veselin Valchev","photoUrl":"","userId":"08341045410101644647"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["import tensorflow as tf"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KU4kjmOx2sSM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}